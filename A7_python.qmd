---
title: "Assignment 7: Smart Beta, Fundamental Indexing, Factor Investing and Stock Returns"
format: html
author: Keyi Wang
date: 10/28/2022
---

# Fundamental Indexing
import packages
```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
from scipy.stats import ttest_ind
import sys
import math
from sklearn import linear_model
from statsmodels.stats.weightstats import DescrStatsW

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")
```
Load compustat dataset and lag the dates by one fiscal year. 
```{python}
cols = ['at','sale','dv','dltis','dltr','sstk','prstkc','gvkey','fyear','datadate','indfmt','datafmt','popsrc','consol','lct','dltt','dlc','seq','lt','scf','ibc','xidoc','dpc','txdc','esubc','sppiv','fopo','fsrco','exre','capx','ivch','aqc','fuseo','sppe','siv','ivstch','ivaco','ni','nopi','spi','act','re','oiadp','prcc_f','csho','dp','pi']
iter_csv = pd.read_csv('C:/Users/kwang648/Downloads/MGT6090/funda.csv', iterator=True, chunksize=1000,usecols = cols)
compustat = pd.concat([chunk for chunk in iter_csv])

compustat.loc[ ( compustat["indfmt"] == 'INDL' ) & ( compustat["datafmt"] == 'STD' ) &( compustat["popsrc"] == 'D') &( compustat["consol"]=='C' ) ]
#convert datadate to datetime
compustat['date'] = pd.to_datetime(compustat['datadate'].astype(str)) + pd.offsets.DateOffset(years = 1)
compustat['month'] =  compustat['date'].dt.month
```
fill nan and filter on rows
```{python}
for col in compustat.loc[:, ~compustat.columns.isin(['gvkey', 'fyear'])]:
    compustat.loc[:,col] = compustat.groupby('gvkey')[col].fillna(method='ffill').fillna(method='bfill')
compustat = compustat.loc[( compustat["at"]-compustat["lct"] != 0 ) & ( ( compustat["dltt"] + compustat["dlc"] + compustat["seq"] ) != 0 ) & ( compustat['sale'] > 0 ) &( compustat['at'] != 0 ) ]
```
calculate fundamental variables
```{python}
#book value = total asset - total liability
compustat['bv'] = compustat['at']-compustat['lt']
#cash flow
compustat.loc[compustat['scf'].isin([1,2,3]),'cf'] = compustat['ibc']+compustat['xidoc']+compustat['dpc']+compustat['txdc']+compustat['esubc']+compustat['sppiv']+compustat['fopo']+compustat['fsrco']
compustat.loc[compustat['scf']==7,'cf'] = compustat['ibc']+compustat['xidoc']+compustat['dpc']+compustat['txdc']+compustat['esubc']+compustat['sppiv']+compustat['fopo']+compustat['exre']
#revenue = sales + non-operating income and special items
compustat['revenue'] = compustat['sale']+compustat['nopi']+compustat['spi']
#investment
compustat.loc[compustat['scf'].isin([1,2,3]),'iv'] = compustat['capx']+compustat['ivch']+compustat['aqc']+compustat['fuseo']-compustat['sppe']-compustat['siv']
compustat.loc[compustat['scf']==7,'iv'] = compustat['capx']+compustat['ivch']+compustat['aqc']-compustat['sppe']-compustat['siv']-compustat['ivstch']-compustat['ivaco']
#profitability
compustat['profitability'] = compustat['ni']/compustat['at']
#turnover = sale/(at(t)+at(t-1))/2
compustat['at_shift'] = compustat.groupby('fyear')['at'].shift()
compustat['turnover'] = compustat['sale']/((compustat['at']+compustat['at_shift'])/2)
```
calculate 5-year trailing average
```{python}
ratios = compustat.groupby(['gvkey','fyear'])['bv'].agg('mean').rolling(5).mean().reset_index()
ratios['cf'] = compustat.groupby(['gvkey','fyear'])['cf'].agg('mean').rolling(5).mean().reset_index()['cf']
ratios['revenue'] = compustat.groupby(['gvkey','fyear'])['revenue'].agg('mean').rolling(5).mean().reset_index()['revenue']
ratios['sale'] = compustat.groupby(['gvkey','fyear'])['sale'].agg('mean').rolling(5).mean().reset_index()['sale']
ratios['dv'] = compustat.groupby(['gvkey','fyear'])['dv'].agg('mean').rolling(5).mean().reset_index()['dv']
ratios['iv'] = compustat.groupby(['gvkey','fyear'])['iv'].agg('mean').rolling(5).mean().reset_index()['iv']
ratios['profitability'] = compustat.groupby(['gvkey','fyear'])['profitability'].agg('mean').rolling(5).mean().reset_index()['profitability']
ratios['turnover'] = compustat.groupby(['gvkey','fyear'])['turnover'].agg('mean').rolling(5).mean().reset_index()['turnover']
```
calculate Altman Z-score
```{python}
compustat['A_z'] = (compustat['act']-compustat['lct'])/compustat['at']
compustat['B_z'] = compustat['re']/compustat['at']
compustat['C_z'] = compustat['oiadp']/compustat['at']
compustat['D_z'] = compustat['prcc_f']*compustat['csho']/compustat['lt']
compustat['E_z'] = compustat['sale']/compustat['at']
compustat['altman_z'] = 1.2*compustat['A_z']+1.4*compustat['B_z']+3.3*compustat['C_z']+0.6*compustat['D_z']+0.99*compustat['E_z']
```
convert to annual altman z-score
```{python}
ratios['altman_z'] = compustat.groupby(['gvkey','fyear'])['altman_z'].agg('mean').reset_index()['altman_z']
```
calculate Ohlson O-score
```{python}
compustat['A_o'] = np.log2(compustat['at'])
compustat['B_o'] = compustat['lt']/compustat['at']
compustat['C_o'] = (compustat['act']-compustat['lct'])/compustat['at']
compustat['D_o'] = compustat['lct']/compustat['act']
compustat['E_o'] = compustat['ni']/compustat['at']
compustat['F_o'] = (compustat['pi']+compustat['dp'])/compustat['lt']
compustat['G_o'] = compustat[['lt','at']].apply(lambda x: 1 if x['lt']>x['at'] else 0,axis=1)
compustat['ni_shift'] = compustat.groupby('fyear')['ni'].shift()
compustat['H_o'] = compustat[['ni','ni_shift']].apply(lambda x: 1 if x['ni']<0 and x['ni_shift']<0 else 0,axis=1)
compustat['I_o'] = (compustat['ni'] - compustat['ni_shift']) / (abs(compustat['ni'])+abs(compustat['ni_shift']))
compustat['ohlson_o'] =  -1.32 -0.407*compustat['A_o'] + 6.03*compustat['B_o'] - 1.43*compustat['C_o'] + 0.0757*compustat['D_o'] - 2.37*compustat['E_o'] - 1.83*compustat['F_o'] - 1.72*compustat['G_o'] + 0.285*compustat['H_o'] - 0.521*compustat['I_o']
```
```{python}
ratios['ohlson_o'] = compustat.groupby(['gvkey','fyear'])['ohlson_o'].agg('mean').reset_index()['ohlson_o']
```
# calculate market variables using CRSP data
here I selected MSF with 12 month of estimation period
load the monthly data and lag the data by one month
```{python}
msf = pd.read_csv('C:/Users/kwang648/Downloads/A6/CRSP_MSF.csv',usecols = ['DATE','VWRETD','RET','PERMNO','HSICCD','PRC','SHROUT','PERMCO'])
msf.rename(columns={'DATE': 'date','VWRETD':'vwretd'}, inplace=True)
for i in msf[['vwretd','RET']]:
    msf[i] = pd.to_numeric(msf[i],errors='coerce')
    msf = msf.dropna()
msf['date'] = pd.to_datetime(msf['date'].astype(str),format='%Y%m%d')+pd.offsets.DateOffset(months = 1)
msf['year'] = pd.DatetimeIndex(msf['date']).year.astype(str)
msf['month'] = pd.DatetimeIndex(msf['date']).month.astype(str)
msf['MKTCAP'] = abs(msf['PRC']) * msf['SHROUT']
```
load monthly risk free rate and combine with msf
```{python}
rf_monthly= pd.read_csv("C:/Users/kwang648/Downloads/A6/KF_data.csv")[['DATE','RF']]
rf_monthly['year'] = rf_monthly['DATE'].str.slice(0,4)
rf_monthly['month'] = rf_monthly['DATE'].str.slice(4,6)
rf_monthly = rf_monthly.drop(['DATE'],axis=1)
rf_monthly.drop(rf_monthly[rf_monthly['RF']=='RF'].index,inplace=True)
rf_monthly['RF'] = pd.to_numeric(rf_monthly['RF'])
msf_rf = rf_monthly.merge(msf,on=['year','month']).drop(['year','month'],axis=1)
```
define function that generates dataset within given period after stock's IPO
```{python}
def return_after_IPO(data,month):
    IPO = data.groupby('PERMNO')['date'].agg('min').reset_index()
    month = int(month)
    #less than 1 year
    if month // 12 == 0:
        end_date = pd.concat([IPO['PERMNO'],IPO['date'] + pd.offsets.DateOffset(months = month)],axis=1)
    if month // 12 > 0:
        year = month / 12
        end_date = pd.concat([IPO['PERMNO'],IPO['date'] + pd.offsets.DateOffset(years = year)],axis=1)
    
    grouped_comp = data.groupby('PERMNO')
    comp_keys = grouped_comp.groups.keys()
    after_IPO = pd.DataFrame()
    for i in comp_keys:
        temp = grouped_comp.get_group(i)
        end = end_date[end_date['PERMNO']==i]['date'].values[0]
        temp = temp[temp['date']<=end]
        after_IPO = pd.concat([after_IPO,temp])
    return after_IPO
```
calculate excess return of each security and market portfolio
```{python}
def calc_excess_return (data):
    data['r_i'] = data['RET']-data['RF']
    data['MKT_i'] = data['vwretd']-data['RF']
    return data
```
```{python}
def linear_regression(data):
    x=data['MKT_i'].to_numpy().reshape(-1,1)
    y=data['r_i'].to_numpy()
    reg = LinearRegression().fit(x,y)
    beta = reg.coef_[0]
    return beta
```
calculate Beta
```{python}
month_12_msf = calc_excess_return(return_after_IPO(msf_rf[pd.DatetimeIndex(msf_rf['date']).year>=1970],12))[['date','r_i','MKT_i','PERMNO']]
beta_12_msf = month_12_msf.groupby([pd.DatetimeIndex(month_12_msf['date']).year,'PERMNO'])[['MKT_i','r_i']].apply(linear_regression).reset_index()
beta_12_msf.columns = ['date','PERMNO','beta_12_msf']
beta_12_msf = beta_12_msf[beta_12_msf['beta_12_msf']!=0]
```
calculate total volatility = standard deviation of stock's return for each year
```{python}
total_volatility = pd.DataFrame(columns = ['date','PERMNO','total_volatility'])
grouped_year = month_12_msf.groupby([pd.DatetimeIndex(month_12_msf['date']).year,'PERMNO'])
year_keys = grouped_year.groups.keys()
for i in year_keys:
    temp = grouped_year.get_group(i)
    mean = temp['r_i'].mean()
    a = math.sqrt(temp['r_i'].apply(lambda x: (x-mean)**2).sum())
    total_volatility.loc[len(total_volatility)]= [i[0],i[1],a]
```
for each year, for each stock i, calculate annualized volatility expressed as a percentage
```{python}
volatility = pd.DataFrame(columns = ['date','PERMNO','volatility'])
grouped = month_12_msf.groupby([pd.DatetimeIndex(month_12_msf['date']).year,'PERMNO'])
keys = grouped.groups.keys()
for i in keys :
    temp = grouped.get_group(i)
    mean = temp['r_i'].mean()
    vol = 100 * math.sqrt(temp['r_i'].agg(lambda x: (x-mean)**2).sum()/(temp['r_i'].agg('count')-1)) * math.sqrt(12)
    volatility.loc[len(volatility)]= [i[0],i[1],vol]
```
another measure of  annualized volatility, assuming Ri = 0
```{python}
volatility_0 = pd.DataFrame(columns = ['date','PERMNO','volatility_0'])
for i in keys:
    temp = grouped.get_group(i)
    vol = 100 * math.sqrt(temp['r_i'].agg(lambda x: (x)**2).sum()/(temp['r_i'].agg('count')-1)) * math.sqrt(12)
    volatility_0.loc[len(volatility_0)]= [i[0],i[1],vol]
```
# Idiosyncratic volatility
import fama-french data and MOM data
```{python}
ff_3_factor = pd.read_csv('C:/Users/kwang648/Downloads/MGT6090/data/fama-french-3-factor.csv')
mom_factor =  pd.read_csv('C:/Users/kwang648/Downloads/MGT6090/data/mom-factor.csv')
ff= ff_3_factor.merge(mom_factor,on='date')
ff['year'] = ff['date'].astype(str).str.slice(0,4)
ff['month'] = ff['date'].astype(str).str.slice(4,6)
```
merge msf with fama-french data
```{python}
msf_ff =calc_excess_return(msf.merge(ff,on=['year','month'])).drop(['MKT_i'],axis=1)
```
run regression on the three models
```{python}
reg_1 = linear_model.LinearRegression().fit(msf_ff['Mkt-RF'].to_numpy().reshape(-1,1),msf_ff['r_i'])
reg_2 = linear_model.LinearRegression().fit(msf_ff[['Mkt-RF','SMB','HML']],msf_ff['r_i'])
reg_3 = linear_model.LinearRegression().fit(msf_ff[['Mkt-RF','SMB','HML','mom']],msf_ff['r_i'])
```
calculate the residuals for the three regressions
```{python}
msf_ff['reg1_residual'] = msf_ff['r_i'] - reg_1.predict(msf_ff['Mkt-RF'].to_numpy().reshape(-1,1))
msf_ff['reg2_residual'] = msf_ff['r_i'] - reg_2.predict(msf_ff[['Mkt-RF','SMB','HML']])
msf_ff['reg3_residual'] = msf_ff['r_i'] - reg_3.predict(msf_ff[['Mkt-RF','SMB','HML','mom']])
```
calculate the residual standard error for each company
```{python}
n=len(np.unique(msf_ff['year']))
rse = msf_ff.groupby(['PERMNO']).apply(lambda x: math.sqrt((x['reg1_residual']**2).sum()/(n-2))).reset_index()
rse.columns = ['PERMNO','rse_1']
rse['rse_2'] = msf_ff.groupby(['PERMNO']).apply(lambda x: math.sqrt((x['reg2_residual']**2).sum()/(n-4))).reset_index()[0]
rse['rse_3'] = msf_ff.groupby(['PERMNO']).apply(lambda x: math.sqrt((x['reg3_residual']**2).sum()/(n-5))).reset_index()[0]
```
calculate Idiosyncratic volatility
```{python}
rse['idiovol_1'] = 100 * rse['rse_1'] * math.sqrt(12)
rse['idiovol_2'] = 100 * rse['rse_2'] * math.sqrt(12)
rse['idiovol_3'] = 100 * rse['rse_3'] * math.sqrt(12)
```
# construct smart beta portfolio using ranking of each fundamental variables
restrict stocks with market capitalization of $100 million
restrict stocks with a stock price greater than $5
```{python}
msf1 = msf[msf['PRC']>5]
mktcap_year = msf.groupby(['PERMCO','year'])['MKTCAP'].sum().reset_index()
mktcap_year = mktcap_year[mktcap_year['MKTCAP']>=100000000]
msf2 = mktcap_year.merge(msf1,on=['year','PERMCO'])
ratios = ratios[ratios['fyear'].astype(int)>=1970]
```
merge the fundamental variables with msf data. 
```{python}
ratios['fyear'] = ratios['fyear'].astype(int)
msf1['year'] = msf1['year'].astype(int)
merge = pd.merge(msf1,ratios,left_on=['PERMCO','year'],right_on=['gvkey','fyear']).drop(['gvkey','fyear'],axis=1)
merge['month'] = merge['month'].astype(int)
```
rank based on fundamental variable of first month
```{python}
def value_weighted_average(x,weight):
    return np.average(x['RET'],weights=x[weight])
```
```{python}
grouped_yr = merge.groupby('year')
yr_key = grouped_yr.groups.keys()
ret = pd.DataFrame(np.unique(merge['year']),columns= ['year'])
for col in merge[['bv','cf','revenue','sale','dv','iv','profitability','turnover','altman_z','ohlson_o']]:
    port_ret = pd.DataFrame(columns = ['year','ret'])
    for i in yr_key:
        temp = grouped_yr.get_group(i)
        temp['rank'] = temp[temp['month']==1][col].rank()
        port_ret.loc[len(port_ret)]= [i,value_weighted_average(temp,col)]
    ret[col] = port_ret['ret']
ret
```
visualize the returnv over time relative to VWRETD returns
```{python}
vwretd = msf2.groupby('year')['vwretd'].mean().reset_index()
for col in ret[['bv','cf','revenue','sale','dv','iv','profitability','turnover','altman_z','ohlson_o']]:
    plt.plot(ret['year'],ret[col])
    plt.plot(vwretd['year'],vwretd['vwretd'])
    plt.show()
```
plot with NBER to see how returns vary with the business cycle
```{python}
nber = pd.read_csv('C:/Users/kwang648/Downloads/MGT6090/data/USREC.csv')
nber['year'] = pd.DatetimeIndex(nber['DATE']).year
nber['month'] = pd.DatetimeIndex(nber['DATE']).month
nber = nber[nber['year'].astype(int)>=1970]
recession = nber.groupby('year')['USREC'].agg('sum').reset_index()
for col in ret[['bv','cf','revenue','sale','dv','iv','profitability','turnover','altman_z','ohlson_o']]:
    plt.plot(ret['year'],ret[col])
    plt.plot(nber['year'],nber['USREC'])
    plt.show()
```
returns = portfolio - rf 
```{python}
def compound(x):
    final = 1
    n=len(x)
    for i in x:
        final=final*(1+i)**(1/n)
    return final-1
```
```{python}
rf = ff[ff['year'].astype(int)>=1970].groupby('year')['RF'].agg(compound).reset_index()
ret1 = pd.DataFrame(np.unique(merge['year']),columns= ['year'])
for col in ret[['bv','cf','revenue','sale','dv','iv','profitability','turnover','altman_z','ohlson_o']]:
    ret1[col] = ret[col] - rf['RF']
ret1
```
excess returns (over VWRETD returns) = portfolio return - vwretd
```{python}
excess_return = pd.DataFrame(np.unique(merge['year']),columns= ['year'])
for col in ret[['bv','cf','revenue','sale','dv','iv','profitability','turnover','altman_z','ohlson_o']]:
    excess_return[col] = ret[col] - vwretd['vwretd']
excess_return
```
voltility, skewness, kurtosis
```{python}
vol = pd.DataFrame(np.unique(merge['year']),columns= ['year'])
skewness = pd.DataFrame(np.unique(merge['year']),columns= ['year'])
kurtosis = pd.DataFrame(np.unique(merge['year']),columns= ['year'])
for col in ret[['bv','cf','revenue','sale','dv','iv','profitability','turnover','altman_z','ohlson_o']]:
    vol1 = pd.DataFrame(columns = ['year','vol'])
    skewness1 = pd.DataFrame(columns = ['year','skewness'])
    kurtosis1 = pd.DataFrame(columns = ['year','kurtosis'])
    for i in yr_key:
        temp = grouped_yr.get_group(i)
        vol2 = temp['RET'].std()
        skewness2 = temp['RET'].skew()
        kurtosis2 = temp['RET'].kurtosis()
        vol1.loc[len(vol1)]= [i,vol2]
        skewness1.loc[len(skewness1)] = [i,skewness2]
        kurtosis1.loc[len(kurtosis1)] = [i,kurtosis2]
    vol[col] = vol1['vol']
    skewness[col] = skewness1['skewness']
    kurtosis[col] = kurtosis1['kurtosis']
vol
```
```{python}
skewness
```
```{python}
kurtosis
```
Sharpe Ratio = return of portfolio - risk free return / volatility of portfolio
information ratio = return of portfolio - vwretd / total volatility
```{python}
sharpe = pd.DataFrame(np.unique(merge['year']),columns= ['year'])
information = pd.DataFrame(np.unique(merge['year']),columns= ['year'])
volatility_all = volatility.groupby('date')['volatility'].mean().reset_index()
volatility_all['volatility'] = volatility_all['volatility']/100
for col in ret[['bv','cf','revenue','sale','dv','iv','profitability','turnover','altman_z','ohlson_o']]:
    sharpe[col] = (ret[col].astype(int)-rf['RF'].astype(int))/(vol[col])
    information[col] = (ret[col] - vwretd['vwretd'])/(volatility_all['volatility'])
sharpe
```
```{python}
information
```
compare return with MKT, SMB, HML from fama-french data
MKT
```{python}
mkt = ff[ff['year'].astype(int)>=1970].groupby('year')['Mkt-RF'].sum().reset_index()
mkt['Mkt-RF'] = mkt['Mkt-RF']/100
smb = ff[ff['year'].astype(int)>=1970].groupby('year')['SMB'].sum().reset_index()
smb['SMB'] = smb['SMB']/100
hml = ff[ff['year'].astype(int)>=1970].groupby('year')['HML'].sum().reset_index()
hml['HML'] = hml['HML']/100
ret_mkt_diff = pd.DataFrame(np.unique(merge['year']),columns= ['year'])
smb_diff= pd.DataFrame(np.unique(merge['year']),columns= ['year'])
hml_diff= pd.DataFrame(np.unique(merge['year']),columns= ['year'])
for col in ret[['bv','cf','revenue','sale','dv','iv','profitability','turnover','altman_z','ohlson_o']]:
    ret_mkt_diff[col] = ret[col]-mkt['Mkt-RF']
    smb_diff[col] = ret[col]-smb['SMB']
    hml_diff[col] = ret[col] - hml['HML']
ret_mkt_diff
```
```{python}
smb_diff
```
```{python}
hml_diff
```
# construct smart beta portfolio using ranking of each market variables
merge the market variables with msf data. 
```{python}
beta_12_msf['date'] = beta_12_msf['date'].astype(int)
volatility['date'] = volatility['date'].astype(int)
total_volatility['date'] = total_volatility['date'].astype(int)
volatility_0['date'] = volatility_0['date'].astype(int)

beta_12_msf['PERMNO'] = beta_12_msf['PERMNO'].astype(int)
volatility['PERMNO'] = volatility['PERMNO'].astype(int)
total_volatility['PERMNO'] = total_volatility['PERMNO'].astype(int)
volatility_0['PERMNO'] = volatility_0['PERMNO'].astype(int)

market_var = beta_12_msf.merge(total_volatility,on=['date','PERMNO']).merge(volatility,on=['date','PERMNO']).merge(volatility_0,on=['date','PERMNO'])
msf2 = msf2[msf2['year']>=1970]
msf_merge = pd.merge(msf2,market_var,left_on=['year','PERMNO'],right_on=['date','PERMNO'])
```
rank based on market variable of first month
```{python}
grouped_yr_m = msf_merge.groupby('year')
yr_key_m = grouped_yr_m.groups.keys()
ret_m = pd.DataFrame(np.unique(msf_merge['year']),columns= ['year'])
for col in msf_merge[['beta_12_msf','total_volatility','volatility','volatility_0']]:
    port_ret_m = pd.DataFrame(columns = ['year','ret'])
    for i in yr_key_m:
        temp = grouped_yr_m.get_group(i)
        temp['rank'] = temp[temp['month']==1][col].rank()
        port_ret_m.loc[len(port_ret_m)]= [i,value_weighted_average(temp,col)]
    ret_m[col] = port_ret_m['ret']
ret_m
```
visualize the return over time relative to VWRETD returns
```{python}
for col in ret_m[['beta_12_msf','total_volatility','volatility','volatility_0']]:
    plt.plot(ret_m['year'],ret_m[col])
    plt.plot(vwretd['year'],vwretd['vwretd'])
    plt.show()
```
plot with NBER to see how returns vary with the business cycle
```{python}
for col in ret_m[['beta_12_msf','total_volatility','volatility','volatility_0']]:
    plt.plot(ret_m['year'],ret_m[col])
    plt.plot(nber['year'],nber['USREC'])
    plt.show()
```
returns = portfolio - rf 
```{python}
ret1_m = pd.DataFrame(np.unique(msf_merge['year']),columns= ['year'])
for col in ret_m[['beta_12_msf','total_volatility','volatility','volatility_0']]:
    ret1_m[col] = ret_m[col] - rf['RF']
ret1_m
```
excess returns (over VWRETD returns) = portfolio return - vwretd
```{python}
excess_return_m = pd.DataFrame(np.unique(msf_merge['year']),columns= ['year'])
for col in ret_m[['beta_12_msf','total_volatility','volatility','volatility_0']]:
    excess_return_m[col] = ret_m[col] - vwretd['vwretd']
excess_return_m
```
voltility, skewness, kurtosis
```{python}
vol_m = pd.DataFrame(np.unique(msf_merge['year']),columns= ['year'])
skewness_m = pd.DataFrame(np.unique(msf_merge['year']),columns= ['year'])
kurtosis_m = pd.DataFrame(np.unique(msf_merge['year']),columns= ['year'])
for col in ret_m[['beta_12_msf','total_volatility','volatility','volatility_0']]:
    vol1 = pd.DataFrame(columns = ['year','vol'])
    skewness1 = pd.DataFrame(columns = ['year','skewness'])
    kurtosis1 = pd.DataFrame(columns = ['year','kurtosis'])
    for i in yr_key_m:
        temp = grouped_yr_m.get_group(i)
        vol2 = temp['RET'].std()
        skewness2 = temp['RET'].skew()
        kurtosis2 = temp['RET'].kurtosis()
        vol1.loc[len(vol1)]= [i,vol2]
        skewness1.loc[len(skewness1)] = [i,skewness2]
        kurtosis1.loc[len(kurtosis1)] = [i,kurtosis2]
    vol_m[col] = vol1['vol']
    skewness_m[col] = skewness1['skewness']
    kurtosis_m[col] = kurtosis1['kurtosis']
vol_m
```
```{python}
skewness_m
```
```{python}
kurtosis_m
```
Sharpe Ratio = return of portfolio - risk free return / volatility of portfolio
information ratio = return of portfolio - vwretd / total volatility
```{python}
sharpe_m = pd.DataFrame(np.unique(msf_merge['year']),columns= ['year'])
information_m = pd.DataFrame(np.unique(msf_merge['year']),columns= ['year'])
for col in ret_m[['beta_12_msf','total_volatility','volatility','volatility_0']]:
    sharpe_m[col] = (ret_m[col].astype(int)-rf['RF'].astype(int))/(vol_m[col])
    information_m[col] = (ret_m[col] - vwretd['vwretd'])/(volatility_all['volatility'])
sharpe_m
```
```{python}
information_m
```
compare return with MKT, SMB, HML from fama-french data
MKT
```{python}
ret_mkt_diff_m = pd.DataFrame(np.unique(msf_merge['year']),columns= ['year'])
smb_diff_m= pd.DataFrame(np.unique(msf_merge['year']),columns= ['year'])
hml_diff_m= pd.DataFrame(np.unique(msf_merge['year']),columns= ['year'])
for col in ret_m[['beta_12_msf','total_volatility','volatility','volatility_0']]:
    ret_mkt_diff_m[col] = ret_m[col]-mkt['Mkt-RF']
    smb_diff_m[col] = ret_m[col]-smb['SMB']
    hml_diff_m[col] = ret_m[col] - hml['HML']
ret_mkt_diff_m
```
```{python}
smb_diff_m
```
```{python}
hml_diff_m
```